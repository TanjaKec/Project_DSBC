---
title: "Assignment"
author: "DS Bootcamp in R"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  html_document:
    theme: yeti
    highlight: kate
    toc: true
    toc_float: true
    df_print: paged
    code_folding: show
---

Write an R Markdown report to address the following problems.

Place the completed report on your GitHub in the "DS_BootCamp" repository, and send the link to it in a direct message via Slack to Tanja.

## **Problem 1**

Explore [`nez-opendata_02_19.csv`](https://tanjakec.github.io/mydata/nez-opendata_02_19.csv) dataset download from [Serbian Open Data portal](https://data.gov.rs/sr/datasets/podatsi-o-saobratshajnim-nezgodama-za-teritoriju-grada-beograda/). This data contains the information about traffic accidents in the territory of the City of Belgrade for the period from January, 01 until February 28, 2019.

In the `traffic_accidents.R` script file this data has been organised for further data wrangling necessary for addressing the problems listed below. 

Note that there are

-	three kind of accidents (variable ` accident`): 

      *	material_demage
      *	deaths and 
      *	injured 
      
-	five different types of accident (variable ` type_acc`):

      *	one_vehicle
      *	two_vehicle_no_turn 
      *	two_vehicle 
      *	parked_vehicle 
      *	pedestrian         


**I)** Use appropriate visualisation to present your findings for the following tasks:
 
   1)	Which kind of the accident is the most frequent?
   2)	Which day in the week has the highest number of accidents?
   3)	For each day in the week find the total number of each kind of accidents
   4)	Identify time in the day (the hour) with the highest number of accidents
   5)	What is the most occurring type of the accident during the hour with the highest number of accidents?

**II)** Creat a ` leafleta` interactive map to pinpoint the accidents with the popup message of the description of the accidents. This popup message should appear every time a user clicks on an accident marker.

**Tips**: for manipulating date-time type of data you might find useful using the [tidyverse]( https://www.tidyverse.org) package [`lubridate`](https://lubridate.tidyverse.org). You might also find useful the check data analysis report about air pollution in Belgrade available from: https://tanjakec.github.io/OH_workshop/OHSA.html

---

## **Problem 2**

**Part I)**

1) Explain what is meant by the following terms:

    i)	Population	
    ii)	Response variable
    iii)	Explanatory Variable	
    iv)	Sample Descriptive Statistics

2) What is the ‘Data Analysis Methodology’, and why is this needed when working with sample data?

3) What are the statistical concepts used to investigate the relationship between a measured response and an attribute explanatory variable? How are these statistical concepts translated into operational procedures for sample data with:
  
  -	an attribute explanatory variable having two levels 
  - an attribute explanatory variable having three or more levels; hence explain when and why a Tukey test is helpful

4) How does multiple regression differ from simple linear regression? In multiple regression how is the contribution of each individual explanatory variable to the regression model initially assessed?

**Part II)**

The [`Fish.csv`](https://tanjakec.github.io/mydata/Fish.csv) dataset is a record of 7 common different fish species in fish market sales This data is available for download from this link: <https://tanjakec.github.io/mydata/Fish.csv>


With this dataset, a predictive model can be performed to estimate the weight of fish. The variables are:

`Species of fish` 

  -	Bream
  -	Parkki
  -	Perch  
  -	Pike     
  -	Roach    
  -	Smelt    
  -	Whitefish 
    
`Weight` weight of fish in Gram g

`Length1` vertical length in cm

`Length2` diagonal length in cm

`Length3` cross length in cm

`Height`  height in cm

`Width` diagonal width in cm


By using an appropriate mix of tests develop a multiple regression model that explains `Weight`. Give a full explanation of your model fitting procedure.

Use Breg and/or Stepwise procedures to build regression models that explains `Weight`. Give a full explanation of the procedure used and interpretation of the outputs.

What do you consider to be the best model and why?

---

## **Problem 3**


Access Lending Club “Bad Loans” dataset from <https://tanjakec.github.io/mydata/Bad_Loans.csv>. The purpose here is to predict whether a loan will be bad (not repaid to the lender). The response column, "bad_loan", is encoded as 1 if the loan was bad, and 0 otherwise. 

1)	Using ML Classification methods: Logistic Regression and KNN train the models for this data to enable prediction of the “bed_loan” variable. 

2)	Compare the accuracy between the trained models. Which one does it perform better?
